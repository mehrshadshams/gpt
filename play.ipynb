{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba63866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ac0ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a0c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34bfcaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4da6def",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B,T,head_size)\n",
    "q = query(x) # (B,T,head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033a88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = q @ k.transpose(-2,-1) * (head_size ** -0.5) # (B,T,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "256981f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
    "v = value(x) # (B,T,head_size)\n",
    "\n",
    "out = wei @ v  # (B,T,C)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cbb50cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
       "        [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
       "        [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f762e2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0287, grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba155f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_hf['transformer.wte.weight'].data_ptr() == sd_hf['lm_head.weight'].data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0eef26",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Panel 1: losses: both train and val\u001b[39;00m\n\u001b[32m     49\u001b[39m plt.subplot(\u001b[32m121\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m xs, ys = \u001b[43mstreams_xy\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# training loss\u001b[39;00m\n\u001b[32m     51\u001b[39m ys = np.array(ys)\n\u001b[32m     52\u001b[39m plt.plot(xs, ys, label=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnanogpt (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msz\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) train loss\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'train'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parse and visualize the logfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sz = \"124M\"\n",
    "\n",
    "loss_baseline = {\n",
    "    \"124M\": 3.2924,\n",
    "}[sz]\n",
    "hella2_baseline = { # HellaSwag for GPT-2\n",
    "    \"124M\": 0.294463,\n",
    "    \"350M\": 0.375224,\n",
    "    \"774M\": 0.431986,\n",
    "    \"1558M\": 0.488946,\n",
    "}[sz]\n",
    "hella3_baseline = { # HellaSwag for GPT-3\n",
    "    \"124M\": 0.337,\n",
    "    \"350M\": 0.436,\n",
    "    \"774M\": 0.510,\n",
    "    \"1558M\": 0.547,\n",
    "}[sz]\n",
    "\n",
    "# load the log file\n",
    "with open(\"log/log.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# parse the individual lines, group by stream (train,val,hella)\n",
    "streams = {}\n",
    "for line in lines:\n",
    "    step, stream, val = line.strip().split()\n",
    "    if stream not in streams:\n",
    "        streams[stream] = {}\n",
    "    streams[stream][int(step)] = float(val)\n",
    "\n",
    "# convert each stream from {step: val} to (steps[], vals[])\n",
    "# so it's easier for plotting\n",
    "streams_xy = {}\n",
    "for k, v in streams.items():\n",
    "    # get all (step, val) items, sort them\n",
    "    xy = sorted(list(v.items()))\n",
    "    # unpack the list of tuples to tuple of lists\n",
    "    streams_xy[k] = list(zip(*xy))\n",
    "\n",
    "# create figure\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Panel 1: losses: both train and val\n",
    "plt.subplot(121)\n",
    "xs, ys = streams_xy[\"train\"] # training loss\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f'nanogpt ({sz}) train loss')\n",
    "print(\"Min Train Loss:\", min(ys))\n",
    "xs, ys = streams_xy[\"val\"] # validation loss\n",
    "plt.plot(xs, ys, label=f'nanogpt ({sz}) val loss')\n",
    "# horizontal line at GPT-2 baseline\n",
    "if loss_baseline is not None:\n",
    "    plt.axhline(y=loss_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint val loss\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale('log')\n",
    "plt.ylim(top=4.0)\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "print(\"Min Validation Loss:\", min(ys))\n",
    "\n",
    "# Panel 2: HellaSwag eval\n",
    "plt.subplot(122)\n",
    "xs, ys = streams_xy[\"hella\"] # HellaSwag eval\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f\"nanogpt ({sz})\")\n",
    "# horizontal line at GPT-2 baseline\n",
    "if hella2_baseline:\n",
    "    plt.axhline(y=hella2_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint\")\n",
    "if hella3_baseline:\n",
    "    plt.axhline(y=hella3_baseline, color='g', linestyle='--', label=f\"OpenAI GPT-3 ({sz}) checkpoint\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"HellaSwag eval\")\n",
    "print(\"Max Hellaswag eval:\", max(ys))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a99da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
